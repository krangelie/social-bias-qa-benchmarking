{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Annotations"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import matplotlib.cm as cm\n",
    "import colorcet as cc\n",
    "from holoviews.plotting.bokeh.styles import font_size\n",
    "from sklearn.metrics import cohen_kappa_score"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "annotations_folder = \".\"",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Plotting helpers"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "cmap = matplotlib.colors.ListedColormap(cc.cm.glasbey.colors[5:])\n",
    "palette = lambda n : sns.color_palette(cc.glasbey, n_colors=n)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def map_list_to_color(lst):\n",
    "    colors = cc.cm.glasbey.colors[5:len(lst)+5]\n",
    "    map = dict(zip(lst, colors))\n",
    "    return map"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Overview of benchmarks"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "benchmarks_overview = pd.read_excel(os.path.join(annotations_folder, \"qa_benchmarks_overview.xlsx\"))\n",
    "benchmarks_overview[\"Year\"].describe()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "benchmarks_overview[\"Year\"].hist()",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Analysis of annotations"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "annotations_survey_results = pd.read_csv(os.path.join(annotations_folder, \"results-survey753164-corrected-readable-titles.tsv\"), sep=\"\\t\")\n",
    "annotations_survey_results.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "annotations_survey_results = annotations_survey_results.loc[~annotations_survey_results[\"bench_abbrev\"].isin([\"newsvqa\", \"newskvqa\"]), :]\n",
    "annotations_survey_results = annotations_survey_results.replace({\"naturalq\": \"naturalquestions\", \"thruthfulqa\": \"truthfulqa\"})"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "open_cols = [col for col in annotations_survey_results.columns if \"comment\" in col] + [\"bench_abbrev\", \"benchname\", \"institution\", \"source_concrete\", \"benchtype\", \"lang\", \"id\"]\n",
    "yes_no_cols = [col for col in annotations_survey_results.columns if col not in open_cols]\n",
    "yes_no_wo_other = [col for col in yes_no_cols if \"other\" not in col]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "(annotations_survey_results[\"bench_abbrev\"].unique())",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "assert len(annotations_survey_results[\"bench_abbrev\"].unique()) == 30",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "pd.set_option(\"future.no_silent_downcasting\", True)\n",
    "#annotations_survey_results = annotations_survey_results.replace({\"Y\": 1, \"N\": 0})\n",
    "annotations_survey_results.loc[:, yes_no_cols] = annotations_survey_results.loc[:, yes_no_cols].fillna(\"N\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Interannotator agreement"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "kappas = []\n",
    "for bench in annotations_survey_results[\"bench_abbrev\"].unique():\n",
    "    annotations_1_and_2 = annotations_survey_results.loc[annotations_survey_results[\"bench_abbrev\"] == bench, yes_no_wo_other]\n",
    "    kappa = cohen_kappa_score(annotations_1_and_2.iloc[0], annotations_1_and_2.iloc[1])\n",
    "    print(bench, kappa)\n",
    "    kappas += [kappa]\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "np.mean(kappas) # Interannotator agreement w/o \"other\" category",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "np.std(kappas)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "annotations_survey_results = annotations_survey_results.replace({\"Y\": 1, \"N\": 0})",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "annotations_survey_results[annotations_survey_results[\"id\"] == 36]",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "external_annotations = annotations_survey_results[annotations_survey_results[\"id\"] <= 36].reset_index()\n",
    "internal_annotations = annotations_survey_results[annotations_survey_results[\"id\"] > 36].reset_index()\n",
    "assert len(external_annotations) == len(internal_annotations)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "benchmarks_wo_human_anno = internal_annotations.loc[internal_annotations[\"anno_how_human\"] == 0, \"bench_abbrev\"].values\n",
    "benchmarks_wo_human_anno"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#value_counts_internal_external = pd.DataFrame(columns=[\"item\", \"internal_0\", \"external_0\",\"internal_1\", \"external_1\"])\n",
    "vc_int_ext_dict = {\"item\": [], \"internal_0\": [], \"external_0\": [],\"internal_1\": [], \"external_1\": []}\n",
    "for col in yes_no_cols:\n",
    "    vc_int_ext_dict[\"item\"] += [col]\n",
    "    if any(map(col.__contains__, [\"identity\", \"recruitment\"])):\n",
    "        # only look at annotator details for benchmarks that involve human annotation\n",
    "        i = internal_annotations.loc[~internal_annotations[\"bench_abbrev\"].isin(benchmarks_wo_human_anno), col].value_counts()\n",
    "        e = external_annotations.loc[~internal_annotations[\"bench_abbrev\"].isin(benchmarks_wo_human_anno), col].value_counts()\n",
    "    else:\n",
    "        i = internal_annotations.loc[:, col].value_counts()\n",
    "        e = external_annotations.loc[:, col].value_counts()\n",
    "    vc_int_ext_dict[\"internal_0\"] += [i[0]] if 0 in i else [0]\n",
    "    vc_int_ext_dict[\"internal_1\"] += [i[1]] if 1 in i else [0]\n",
    "    vc_int_ext_dict[\"external_0\"] += [e[0]] if 0 in e else [0]\n",
    "    vc_int_ext_dict[\"external_1\"] += [e[1]] if 1 in e else [0]\n",
    "vc_int_ext_df = pd.DataFrame(vc_int_ext_dict)\n",
    "vc_int_ext_df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Detailed analysis of internal and external annotation differences"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def print_diff_for_col(col):\n",
    "    for b in internal_annotations[\"bench_abbrev\"].unique():\n",
    "        int = internal_annotations.loc[internal_annotations[\"bench_abbrev\"] == b, col].values\n",
    "        ext = external_annotations.loc[external_annotations[\"bench_abbrev\"] == b, col].values\n",
    "        if int != ext:\n",
    "            print(b)\n",
    "            print(\"Internal: \", int,\n",
    "                  internal_annotations.loc[internal_annotations[\"bench_abbrev\"] == b, col + \"_comment\"].values)\n",
    "            print(\"External: \", ext,\n",
    "                  external_annotations.loc[external_annotations[\"bench_abbrev\"] == b, col + \"_comment\"].values)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for col in yes_no_cols:\n",
    "    print(col)\n",
    "    print_diff_for_col(col)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Reported annotator demographics"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "identity_demographic_color_map = map_list_to_color([col for col in internal_annotations.columns if \"identity_demographic\" in col and not \"comment\" in col])",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "internal_annotations.loc[internal_annotations[\"identity_demographic_none\"] == 0, [\"bench_abbrev\"]+[col for col in internal_annotations.columns if \"identity_demographic\" in col]]",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "external_annotations.loc[external_annotations[\"identity_demographic_none\"] == 0, [\"bench_abbrev\"]+[col for col in internal_annotations.columns if \"identity_demographic\" in col]]",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data = vc_int_ext_df.loc[(vc_int_ext_df[\"item\"].str.contains(\"identity_demographic\")) & (vc_int_ext_df[\"internal_1\"] > 0), [\"item\", \"internal_1\"]]\n",
    "data = data.sort_values(by=\"internal_1\")\n",
    "plt.figure(figsize=(6,6))\n",
    "sns.set_style(\"whitegrid\")\n",
    "relabel_map = {\"edu\": \"education\", \"domain\": \"area of expertise\", \"recruitment_country\": \"recruitment country\", \"origin_country\": \"country of origin\"}\n",
    "x = data.internal_1\n",
    "colors = [identity_demographic_color_map[l] for l in data.item]\n",
    "labels = [label.replace(\"identity_demographic_\", \"\") for label in data.item]\n",
    "labels = [relabel_map[l] if l in relabel_map else l for l in labels]\n",
    "patches, texts, autotexts = plt.pie(x, labels=labels, autopct=(lambda p: '{:.0f}'.format(p * 30 / 100)), colors=colors)\n",
    "for txt in texts:\n",
    "    txt.set_fontsize(16)\n",
    "for autotext in autotexts:\n",
    "    autotext.set_color('white')\n",
    "    autotext.set_fontsize(18)\n",
    "# Add a title\n",
    "plt.title(\"Reported demographic information about annotators\", fontsize=20)\n",
    "plt.get_figlabels()\n",
    "# Display the plot\n",
    "plt.savefig(os.path.join(annotations_folder, \"images\", 'identity_demographic.pdf'), format='pdf', dpi=300, bbox_inches='tight')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data = vc_int_ext_df.loc[(vc_int_ext_df[\"item\"].str.contains(\"identity_demographic\")) & (vc_int_ext_df[\"external_1\"] > 0), [\"item\", \"external_1\"]]\n",
    "data = data.sort_values(by=\"external_1\")\n",
    "# Create our pie chart with labels\n",
    "plt.figure(figsize=(6,6))\n",
    "sns.set_style(\"whitegrid\")\n",
    "relabel_map = {\"edu\": \"education\", \"domain\": \"area of expertise\", \"recruitment_country\": \"recruitment country\", \"origin_country\": \"country of origin\"}\n",
    "x = data.external_1\n",
    "colors = [identity_demographic_color_map[l] for l in data.item]\n",
    "labels = [label.replace(\"identity_demographic_\", \"\") for label in data.item]\n",
    "labels = [relabel_map[l] if l in relabel_map else l for l in labels]\n",
    "patches, texts, autotexts = plt.pie(x, labels=labels, autopct=(lambda p: '{:.0f}'.format(p * 30 / 100)), colors=colors)\n",
    "for txt in texts:\n",
    "    txt.set_fontsize(16)\n",
    "for autotext in autotexts:\n",
    "    autotext.set_color('white')\n",
    "    autotext.set_fontsize(18)\n",
    "# Add a titletotal\n",
    "plt.title(\"Reported demographic information about annotators\\n(external annotations)\", fontsize=20)\n",
    "\n",
    "# Display the plot\n",
    "plt.savefig(os.path.join(annotations_folder, \"images\", 'identity_demographic_ext.pdf'), format='pdf', dpi=300, bbox_inches='tight')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Language of benchmark"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "internal_annotations.loc[internal_annotations[\"lang\"] != \"English\", \"bench_abbrev\"]",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "external_annotations.loc[external_annotations[\"lang\"] != \"English\", \"bench_abbrev\"]",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "external_annotations.loc[external_annotations[\"bench_abbrev\"] == \"okvqa\", \"lang\"]\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## How was the data annotated?"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "internal_annotations[\"anno_how_human\"].value_counts()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "external_annotations[\"anno_how_human\"].value_counts()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "internal_annotations.loc[internal_annotations[\"anno_how_human\"] == 0, \"bench_abbrev\"]",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "external_annotations.loc[external_annotations[\"anno_how_human\"] == 0, \"bench_abbrev\"]",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "external_annotations.loc[external_annotations[\"bench_abbrev\"].isin([\"arc\", \"scienceqa\", \"xquad\"]), \"anno_how_human_comment\"].values",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Detailed look at anntotator disagreement"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print_diff_for_col(\"anno_how_human\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Recruitment criteria"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "internal_annotations.loc[(internal_annotations[\"recruitment_criteria_none\"] == 0) & (internal_annotations[\"anno_how_human\"] == 1), \"bench_abbrev\"]",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "internal_annotations.loc[(internal_annotations[\"recruitment_criteria_none\"] == 0) & (internal_annotations[\"anno_how_human\"] == 1), \"bench_abbrev\"]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "recruitment_criteria_labels = [col for col in internal_annotations.columns if \"recruitment_criteria\" in col]\n",
    "recruitment_criteria_labels"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "internal_annotations.loc[internal_annotations[\"bench_abbrev\"] == \"coqa\", recruitment_criteria_labels]",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "internal_annotations.loc[(internal_annotations[\"anno_how_human\"] == 1), [\"bench_abbrev\"] + recruitment_criteria_labels ]",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "external_annotations.loc[(external_annotations[\"recruitment_criteria_none\"] == 0) & (external_annotations[\"anno_how_human\"] == 1), [\"bench_abbrev\"] + recruitment_criteria_labels ]",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "vc_int_ext_df.loc[vc_int_ext_df[\"item\"].str.contains(\"recruitment_criteria\"), [\"item\", \"external_1\"]]",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "vc_int_ext_df.loc[vc_int_ext_df[\"item\"].str.contains(\"recruitment_criteria\"), [\"item\", \"internal_1\"]]",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "identity_demographic_color_map",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "recruitment_criteria_color_map = map_list_to_color([col for col in internal_annotations.columns if \"recruitment_criteria\" in col and not \"comment\" in col])\n",
    "recruitment_criteria_color_map"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "recruitment_criteria_color_map[\"recruitment_criteria_other\"] = identity_demographic_color_map[\"identity_demographic_other\"]",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data = vc_int_ext_df.loc[(vc_int_ext_df[\"item\"].str.contains(\"recruitment_criteria\")) & (vc_int_ext_df[\"internal_1\"] > 0), [\"item\", \"internal_1\"]]\n",
    "data = data.sort_values(by=\"internal_1\")\n",
    "data.loc[data[\"item\"] == \"recruitment_criteria_task\", \"internal_1\"] += data.loc[data[\"item\"] == \"recruitment_criteria_rank\", \"internal_1\"].values\n",
    "data = data.loc[data[\"item\"] != \"recruitment_criteria_rank\", :]\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "sns.set_style(\"whitegrid\")\n",
    "relabel_map = {\"available\": \"availability\", \"task\": \"task performance\", \"expertise\": \"domain expertise\"}\n",
    "x = data.internal_1\n",
    "colors = [recruitment_criteria_color_map[l] for l in data.item]\n",
    "labels = [label.replace(\"recruitment_criteria_\", \"\") for label in data.item]\n",
    "labels = [relabel_map[l] if l in relabel_map else l for l in labels]\n",
    "patches, texts, autotexts = plt.pie(x, labels=labels, autopct=(lambda p: '{:.0f}'.format(p * 30 / 100)), colors=colors)\n",
    "for txt in texts:\n",
    "    txt.set_fontsize(16)\n",
    "for autotext in autotexts:\n",
    "    autotext.set_color('white')\n",
    "    autotext.set_fontsize(18)\n",
    "# Add a title\n",
    "plt.title(\"Reported annotator recruitment criteria\", fontsize=20)\n",
    "plt.get_figlabels()\n",
    "# Display the plot\n",
    "plt.savefig(os.path.join(annotations_folder, \"images\", 'recruitment_criteria.pdf'), format='pdf', dpi=300, bbox_inches='tight')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data = vc_int_ext_df.loc[(vc_int_ext_df[\"item\"].str.contains(\"recruitment_criteria\")) & (vc_int_ext_df[\"external_1\"] > 0), [\"item\", \"external_1\"]]\n",
    "data = data.sort_values(by=\"external_1\")\n",
    "data.loc[data[\"item\"] == \"recruitment_criteria_task\", \"external_1\"] += data.loc[data[\"item\"] == \"recruitment_criteria_rank\", \"external_1\"].values\n",
    "data = data.loc[data[\"item\"] != \"recruitment_criteria_rank\", :]\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "sns.set_style(\"whitegrid\")\n",
    "relabel_map = {\"available\": \"availability\", \"task\": \"task performance\", \"expertise\": \"domain expertise\"}\n",
    "x = data.external_1\n",
    "colors = [recruitment_criteria_color_map[l] for l in data.item]\n",
    "labels = [label.replace(\"recruitment_criteria_\", \"\") for label in data.item]\n",
    "labels = [relabel_map[l] if l in relabel_map else l for l in labels]\n",
    "patches, texts, autotexts = plt.pie(x, labels=labels, autopct=(lambda p: '{:.0f}'.format(p * 30 / 100)), colors=colors)\n",
    "for txt in texts:\n",
    "    txt.set_fontsize(16)\n",
    "for autotext in autotexts:\n",
    "    autotext.set_color('white')\n",
    "    autotext.set_fontsize(18)\n",
    "# Add a title\n",
    "plt.title(\"Reported annotator recruitment criteria\\n(external annotations)\", fontsize=20)\n",
    "plt.get_figlabels()\n",
    "# Display the plot\n",
    "plt.savefig(os.path.join(annotations_folder, \"images\", 'recruitment_criteria_ext.pdf'), format='pdf', dpi=300, bbox_inches='tight')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Are the contents made transparent in the reports?"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "internal_annotations[\"content_none\"].value_counts()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "content_labels = [col for col in internal_annotations.columns if \"content\" in col]\n",
    "content_labels"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "for col in content_labels:\n",
    "    if \"comment\" not in col:\n",
    "        print(internal_annotations[col].value_counts())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What contents are found?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "knowledge_type_labels = [col for col in internal_annotations.columns if \"knowledge_type\" in col]\n",
    "knowledge_type_labels"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "for col in knowledge_type_labels:\n",
    "    if \"comment\" not in col:\n",
    "        print(internal_annotations[col].value_counts())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "knowledge_type_labels_map = {\"knowledge_type_news\": \"news/entertainment/pop culture\", \n",
    "                             \"knowledge_type_everyday\": \"everyday/world knowledge\",\n",
    "                             \"knowledge_type_edu\": \"education\",\n",
    "                             \"knowledge_type_art\": \"art/design/music\",\n",
    "                             \"knowledge_type_maths\": \"maths\",\n",
    "                             \"knowledge_type_lang\": \"language/linguistics\",\n",
    "                             \"knowledge_type_commonsense\": \"commonsense\",\n",
    "                             \"knowledge_type_encycl\": \"encyclopedic\",\n",
    "                             \"knowledge_type_humanities\": \"humanities\",\n",
    "                             \"knowledge_type_sosci\": \"social science\",\n",
    "                             \"knowledge_type_stem\": \"science/technology/engineering\",\n",
    "                             \"knowledge_type_medicine\": \"medicine/health\",\n",
    "                             \"knowledge_type_business\": \"business/economics/finance\",\n",
    "                             \"knowledge_type_other\": \"other\"}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import matplotlib.ticker as mticker\n",
    "plt.figure(figsize=(5,3))\n",
    "knowledge_type_counts = []\n",
    "knowledge_type_labels_no_comment = []\n",
    "for col in knowledge_type_labels:\n",
    "    if (\"comment\" not in col) and (\"none\" not in col):\n",
    "        knowledge_type_counts += [sum(external_annotations[col] == 1)]\n",
    "        knowledge_type_labels_no_comment += [knowledge_type_labels_map[col]]\n",
    "sorted_lists = sorted(zip(knowledge_type_counts, knowledge_type_labels_no_comment), reverse=True)\n",
    "knowledge_type_counts, knowledge_type_labels_no_comment = zip(*sorted_lists)\n",
    "sns.barplot(x=np.array(knowledge_type_counts), y=np.array(knowledge_type_labels_no_comment), color=cc.cm.glasbey.colors[10] )\n",
    "plt.gca().xaxis.set_major_locator(mticker.MultipleLocator(5))\n",
    "plt.title(\"Domains (external annotations)\")\n",
    "plt.tight_layout()\n",
    "os.makedirs(\"../images\", exist_ok=True)\n",
    "plt.savefig(os.path.join(annotations_folder, \"images\", \"knowledge_types_ext.pdf\"), dpi=300)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import matplotlib.ticker as mticker\n",
    "plt.figure(figsize=(5,3))\n",
    "knowledge_type_counts = []\n",
    "knowledge_type_labels_no_comment = []\n",
    "for col in knowledge_type_labels:\n",
    "    if (\"comment\" not in col) and (\"none\" not in col):\n",
    "        knowledge_type_counts += [sum(internal_annotations[col] == 1)]\n",
    "        knowledge_type_labels_no_comment += [knowledge_type_labels_map[col]]\n",
    "sorted_lists = sorted(zip(knowledge_type_counts, knowledge_type_labels_no_comment), reverse=True)\n",
    "knowledge_type_counts, knowledge_type_labels_no_comment = zip(*sorted_lists)\n",
    "sns.barplot(x=np.array(knowledge_type_counts), y=np.array(knowledge_type_labels_no_comment), color=cc.cm.glasbey.colors[10])\n",
    "plt.gca().xaxis.set_major_locator(mticker.MultipleLocator(5))\n",
    "plt.title(\"Domains (internal annotations)\")\n",
    "plt.tight_layout()\n",
    "os.makedirs(\"../images\", exist_ok=True)\n",
    "plt.savefig(os.path.join(annotations_folder, \"images\", \"knowledge_types.pdf\"), dpi=300)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "internal_annotations[\"bench_abbrev\"]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "internal_annotations.columns[:15]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "bench = \"scienceqa\"\n",
    "for col in knowledge_type_labels:\n",
    "    if (\"comment\" not in col) and (\"none\" not in col):\n",
    "        val = internal_annotations.loc[internal_annotations[\"bench_abbrev\"] == bench, col]\n",
    "        if val.values[0] != 0:\n",
    "            print(col)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "for col in knowledge_type_labels:\n",
    "    if (\"comment\" not in col) and (\"none\" not in col):\n",
    "        val = external_annotations.loc[external_annotations[\"bench_abbrev\"] == bench, col]\n",
    "        if val.values[0] != 0:\n",
    "            print(col)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "internal_annotations.loc[internal_annotations[\"bench_abbrev\"] == bench, \"knowledge_type_other_comment\"]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Data source"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "internal_annotations.loc[internal_annotations[\"bench_abbrev\"] == bench, \"source_concrete\"].values"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "external_annotations.loc[external_annotations[\"bench_abbrev\"] == bench, \"source_concrete\"].values"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## What are the institutions?"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "all_inst_int = []\n",
    "for b in internal_annotations[\"bench_abbrev\"].values:\n",
    "    all_inst_int += internal_annotations.loc[internal_annotations[\"bench_abbrev\"] == b, \"institution\"].values[0].split(\", \")\n",
    "pd.Series(all_inst_int).value_counts()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "all_inst_ext = []\n",
    "for b in external_annotations[\"bench_abbrev\"].values:\n",
    "    all_inst_ext += external_annotations.loc[external_annotations[\"bench_abbrev\"] == b, \"institution\"].values[0].split(\", \")\n",
    "pd.Series(all_inst_ext).value_counts()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## How was the data sourced?"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "source_how_labels = [col for col in internal_annotations.columns if \"source_how\" in col]",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "vc_int_ext_df.loc[(vc_int_ext_df[\"item\"].str.contains(\"source_how\")), :]",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "internal_annotations.loc[internal_annotations[\"source_how_web\"] == 1, [\"bench_abbrev\", \"source_how_web_comment\"]]\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "external_annotations.loc[external_annotations[\"source_how_web\"] == 1, [\"bench_abbrev\", \"source_how_web_comment\"]].values",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "external_annotations.loc[external_annotations[\"bench_abbrev\"] == \"copa\", \"source_how_dataset_comment\"].values\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "sns.set_style(\"white\")\n",
    "data = vc_int_ext_df.loc[\n",
    "    (vc_int_ext_df[\"item\"].str.contains(\"source_how\")), [\"item\", \"internal_1\"]].sort_values(\"internal_1\", ascending=False)\n",
    "relabel_map = {\"human\": \"human-authored\",\n",
    "               \"web\": \"open access/web data\",\n",
    "               \"dataset\": \"reuse of existing AI/NLP dataset\",\n",
    "               \"exam\": \"exams or textbooks\",\n",
    "               \"private\": \"proprietary/internal source\"}\n",
    "data[\"item\"] = data[\"item\"].map(lambda l: l.replace(\"source_how_\", \"\"))\n",
    "data[\"item\"] = data[\"item\"].map(lambda l: relabel_map[l] if l in relabel_map else l)\n",
    "#labels = [label.replace(\"goal_\", \"\") for label in data.item]\n",
    "#labels = [relabel_map[l] if l in relabel_map else l for l in labels]\n",
    "g = sns.catplot(\n",
    "    data=data, kind=\"bar\",\n",
    "    x=\"internal_1\", y=\"item\",  width=0.8,\n",
    "    color=cc.cm.glasbey.colors[10]\n",
    ")\n",
    "g.despine(left=True)\n",
    "g.set_axis_labels(\"Count\", \"\")\n",
    "plt.xticks(np.arange(0, 21, 5))\n",
    "plt.title(\"Reported data source\", fontsize=12)\n",
    "plt.gcf().set_size_inches(4,2)\n",
    "plt.savefig(os.path.join(annotations_folder, \"images\", 'data_collection.pdf'), format='pdf', dpi=300, bbox_inches='tight')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "sns.set_style(\"white\")\n",
    "data = vc_int_ext_df.loc[\n",
    "    (vc_int_ext_df[\"item\"].str.contains(\"source_how\")), [\"item\", \"external_1\"]].sort_values(\"external_1\", ascending=False)\n",
    "relabel_map = {\"human\": \"human-authored\",\n",
    "               \"web\": \"open access/web data\",\n",
    "               \"dataset\": \"reuse of existing AI/NLP dataset\",\n",
    "               \"exam\": \"exams or textbooks\",\n",
    "               \"private\": \"proprietary/internal source\"}\n",
    "data[\"item\"] = data[\"item\"].map(lambda l: l.replace(\"source_how_\", \"\"))\n",
    "data[\"item\"] = data[\"item\"].map(lambda l: relabel_map[l] if l in relabel_map else l)\n",
    "#labels = [label.replace(\"goal_\", \"\") for label in data.item]\n",
    "#labels = [relabel_map[l] if l in relabel_map else l for l in labels]\n",
    "g = sns.catplot(\n",
    "    data=data, kind=\"bar\",\n",
    "    x=\"external_1\", y=\"item\", width=0.8,\n",
    "    color=cc.cm.glasbey.colors[10]\n",
    ")\n",
    "g.despine(left=True)\n",
    "g.set_axis_labels(\"Count\", \"\")\n",
    "plt.xticks(np.arange(0, 21, 5))\n",
    "plt.title(\"Reported data source\\n(external annotation)\", fontsize=12)\n",
    "plt.gcf().set_size_inches(4,2)\n",
    "plt.savefig(os.path.join(annotations_folder, \"images\", 'data_collection_ext.pdf'), format='pdf', dpi=300, bbox_inches='tight')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Topics/domains"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "internal_annotations.loc[internal_annotations[\"knowledge_type_other\"] == 1, \"knowledge_type_other_comment\"]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "external_annotations.loc[external_annotations[\"knowledge_type_other\"] == 1, \"knowledge_type_other_comment\"]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Publication years"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "benchmarks_overview = benchmarks_overview.rename({\"Benchmark\": \"bench_abbrev\"}, axis=1)\n",
    "benchmarks_overview.columns"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "internal_plus_year = internal_annotations.merge(benchmarks_overview, on=\"bench_abbrev\", how='left')\n",
    "internal_plus_year.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "knowledge_type_year = []\n",
    "knowledge_type_labels_no_comment = []\n",
    "for col in knowledge_type_labels:\n",
    "    if (\"comment\" not in col) and (\"none\" not in col):\n",
    "        years = internal_plus_year.loc[internal_plus_year[col]==1, \"Year\"].to_list()\n",
    "        knowledge_type_year += years\n",
    "        knowledge_type_labels_no_comment += [knowledge_type_labels_map[col]] * len(years)\n",
    "sorted_lists = sorted(zip(knowledge_type_year, knowledge_type_labels_no_comment), reverse=True)\n",
    "internal_plus_year_df = pd.DataFrame(sorted_lists, columns=[\"year\", \"domain\"])\n",
    "internal_plus_year_df = internal_plus_year_df.dropna()\n",
    "#knowledge_type_year, knowledge_type_labels_no_comment = zip(*sorted_lists)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "internal_plus_year.loc[:, [\"Year\", \"bench_abbrev\"]].sort_values(by=\"Year\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "internal_plus_year_df.groupby(['year']).value_counts()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "internal_plus_year_counted_df = internal_plus_year_df.groupby(['year']).value_counts().reset_index().rename(columns={\"index\": \"year\", 0: \"count\"})\n",
    "internal_plus_year_counted_df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "plt.figure(figsize=((7,5)))\n",
    "sns.scatterplot(data=internal_plus_year_counted_df, x=\"year\", y=\"count\", hue=\"domain\")\n",
    "plt.tight_layout()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Bias & toxicity"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print_diff_for_col(\"bias_toxicity_none\")\n",
    "external_annotations.loc[external_annotations[\"bench_abbrev\"] == \"naturalquestions\", [col for col in external_annotations.columns if \"bias\" in col]]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "sum(internal_annotations[\"bias_toxicity_none\"])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "sum(external_annotations[\"bias_toxicity_none\"])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "internal_annotations[internal_annotations[\"bias_toxicity_toxic\"]==1]\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "external_annotations[external_annotations[\"bias_toxicity_toxic\"]==1]",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Benchmark goal"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "for col in goal_labels:\n",
    "    print(col, sum(internal_annotations[col]))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "goal_labels = [col for col in internal_annotations.columns if \"goal\" in col and not \"comment\" in col]",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "for col in goal_labels:\n",
    "    print(col, sum(external_annotations[col]))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "goals_color_map = map_list_to_color(goal_labels)\n",
    "data = vc_int_ext_df.loc[\n",
    "    (vc_int_ext_df[\"item\"].str.contains(\"goal\")) & (vc_int_ext_df[\"internal_1\"] > 0), [\"item\", \"internal_1\"]]\n",
    "data = data.sort_values(by=\"internal_1\")\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.set_style(\"whitegrid\")\n",
    "relabel_map = {\"new_task\": \"to define a new task\", \"realistic\": \"more realistic questions compared to existing benchmarks\", \"difficulty\": \"increased difficulty compared to existing benchmarks\"}\n",
    "x = data.internal_1\n",
    "colors = [goals_color_map[l] for l in data.item]\n",
    "labels = [label.replace(\"goal_\", \"\") for label in data.item]\n",
    "labels = [relabel_map[l] if l in relabel_map else l for l in labels]\n",
    "patches, texts, autotexts = plt.pie(x, labels=labels, autopct=(lambda p: '{:.0f}'.format(p * 30 / 100)), colors=colors)\n",
    "for txt in texts:\n",
    "    txt.set_fontsize(16)\n",
    "for autotext in autotexts:\n",
    "    autotext.set_color('white')\n",
    "    autotext.set_fontsize(16)\n",
    "# Add a title\n",
    "plt.title(\"Reported motivation\", fontsize=18)\n",
    "plt.get_figlabels()\n",
    "# Display the plot\n",
    "plt.savefig(os.path.join(annotations_folder, \"images\", 'motivation.pdf'), format='pdf', dpi=300,\n",
    "            bbox_inches='tight')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "int_sorted = vc_int_ext_df.loc[vc_int_ext_df[\"item\"].str.contains(\"goal\"), [\"item\", \"internal_1\"]].sort_values(\"internal_1\", ascending=False)\n",
    "int_sorted"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ext_sorted = vc_int_ext_df.loc[vc_int_ext_df[\"item\"].str.contains(\"goal\"), [\"item\", \"external_1\"]].sort_values(\"external_1\", ascending=False)\n",
    "ext_sorted"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "internal_annotations.loc[internal_annotations[\"goal_other\"] > 0, [\"bench_abbrev\", \"goal_other_comment\"]]",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "external_annotations.loc[external_annotations[\"goal_represent\"] > 0, [\"bench_abbrev\", \"goal_represent_comment\"]].values[0]\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "sns.set_style(\"white\")\n",
    "data = vc_int_ext_df.loc[\n",
    "    (vc_int_ext_df[\"item\"].str.contains(\"goal\")), [\"item\", \"internal_1\"]].sort_values(\"internal_1\", ascending=False)\n",
    "relabel_map = {\"new_task\": \"to define a new task\",\n",
    "               \"realistic\": \"more realistic questions\",\n",
    "               \"difficulty\": \"increased difficulty\",\n",
    "               \"less_difficulty\": \"decreased difficulty\",\n",
    "               \"represent\": \"better social representativeness\"}\n",
    "data[\"item\"] = data[\"item\"].map(lambda l: l.replace(\"goal_\", \"\"))\n",
    "data[\"item\"] = data[\"item\"].map(lambda l: relabel_map[l] if l in relabel_map else l)\n",
    "#labels = [label.replace(\"goal_\", \"\") for label in data.item]\n",
    "#labels = [relabel_map[l] if l in relabel_map else l for l in labels]\n",
    "g = sns.catplot(\n",
    "    data=data, kind=\"bar\",\n",
    "    x=\"internal_1\", y=\"item\", width=0.8,\n",
    "    color=cc.cm.glasbey.colors[10]\n",
    ")\n",
    "g.despine(left=True)\n",
    "g.set_axis_labels(\"Count\", \"\")\n",
    "plt.xticks(np.arange(0, 21, 5))\n",
    "plt.title(\"Reported motivation\", fontsize=12)\n",
    "plt.gcf().set_size_inches(4,1.7)\n",
    "plt.savefig(os.path.join(annotations_folder, \"images\", 'motivation.pdf'), format='pdf', dpi=300, bbox_inches='tight')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "sns.set_style(\"white\")\n",
    "data = vc_int_ext_df.loc[\n",
    "    (vc_int_ext_df[\"item\"].str.contains(\"goal\")), [\"item\", \"external_1\"]].sort_values(\"external_1\", ascending=False)\n",
    "relabel_map = {\"new_task\": \"to define a new task\",\n",
    "               \"realistic\": \"more realistic questions\",\n",
    "               \"difficulty\": \"increased difficulty\",\n",
    "               \"less_difficulty\": \"decreased difficulty\",\n",
    "               \"represent\": \"better social representativeness\"}\n",
    "data[\"item\"] = data[\"item\"].map(lambda l: l.replace(\"goal_\", \"\"))\n",
    "data[\"item\"] = data[\"item\"].map(lambda l: relabel_map[l] if l in relabel_map else l)\n",
    "#labels = [label.replace(\"goal_\", \"\") for label in data.item]\n",
    "#labels = [relabel_map[l] if l in relabel_map else l for l in labels]\n",
    "g = sns.catplot(\n",
    "    data=data, kind=\"bar\",\n",
    "    x=\"external_1\", y=\"item\", width=0.8,\n",
    "    color=cc.cm.glasbey.colors[10]\n",
    ")\n",
    "g.despine(left=True)\n",
    "g.set_axis_labels(\"Count\", \"\")\n",
    "plt.xticks(np.arange(0, 21, 5))\n",
    "\n",
    "plt.title(\"Reported motivation\\n(external annotation)\", fontsize=12)\n",
    "plt.gcf().set_size_inches(4,1.7)\n",
    "plt.savefig(os.path.join(annotations_folder, \"images\", 'motivation_ext.pdf'), format='pdf', dpi=300, bbox_inches='tight')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "relabel_map = {\"human\": \"human-authored\",\n",
    "               \"web\": \"open access/web data\",\n",
    "               \"dataset\": \"reuse of existing AI/NLP dataset\",\n",
    "               \"exam\": \"exams or textbooks\",\n",
    "               \"private\": \"proprietary/internal source\"}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": ""
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Transparent benchmarks"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "none_cols = [col for col in internal_annotations.columns if \"none\" in col]\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "none_data_no_comments_cols = [col for col in none_cols if \"comment\" not in col]\n",
    "len(none_data_no_comments_cols)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "benchmarks = internal_annotations[\"bench_abbrev\"].values\n",
    "benchmarks"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for b in benchmarks:\n",
    "    sum_none = internal_annotations.loc[internal_annotations[\"bench_abbrev\"] == b, none_data_no_comments_cols].sum(axis=1)\n",
    "    print(b, sum_none.values)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for b in benchmarks:\n",
    "    sum_none = external_annotations.loc[external_annotations[\"bench_abbrev\"] == b, none_data_no_comments_cols].sum(axis=1)\n",
    "    print(b, sum_none.values)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "internal_annotations.loc[internal_annotations[\"identity_demographic_none\"] == 0, \"bench_abbrev\"].values",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "internal_annotations.loc[internal_annotations[\"recruitment_criteria_none\"] == 0, \"bench_abbrev\"].values\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "internal_annotations.loc[internal_annotations[\"bias_toxicity_none\"] == 0, \"bench_abbrev\"].values\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
